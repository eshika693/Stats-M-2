{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Ques1) What is a random variable in probability theory?"
      ],
      "metadata": {
        "id": "avbW_yUUZQ4S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- In probability theory, a random variable is a numerical quantity whose value depends on the outcome of a random phenomenon. It provides a way to map outcomes of a probabilistic experiment to numbers, which allows for mathematical analysis. There are two main types of random variables: discrete and continuous. A discrete random variable takes on a countable number of possible values (like the result of a dice roll), while a continuous random variable can take on any value within a given range (like the exact height of a person). Although the outcomes themselves may be unpredictable, random variables allow us to study the distribution and behavior of these outcomes using probability.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "genFRYO_ZUJi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques2) What are the types of random variables?"
      ],
      "metadata": {
        "id": "v413ccFMZbir"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Random variables are primarily classified into two types: discrete and continuous. A discrete random variable takes on a finite or countably infinite set of distinct values. Examples include the number of heads in a series of coin tosses or the roll of a die, where the possible outcomes are specific and separate. On the other hand, a continuous random variable can assume any value within a given interval or range, including fractions and irrational numbers. Examples include measurements such as height, weight, or time, where the possible values form a continuum. These two types differ not only in the kind of values they can take but also in the methods used to analyze them, such as probability mass functions for discrete variables and probability density functions for continuous ones."
      ],
      "metadata": {
        "id": "-aF37FSJZfXI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques3) What is the difference between discrete and continuous distributions?"
      ],
      "metadata": {
        "id": "8qVl_r-hZop1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The main difference between discrete and continuous distributions lies in the types of values their random variables can assume and how probabilities are assigned. A discrete distribution is associated with a discrete random variable and describes the probability of each possible distinct outcome. The probabilities are typically presented using a probability mass function (PMF), where the sum of all probabilities equals one. In contrast, a continuous distribution is linked to a continuous random variable and assigns probabilities over an interval of values rather than individual points. Since there are infinitely many possible values, the probability of any single value is zero, and probabilities are instead determined using a probability density function (PDF), where the area under the curve over a range represents the likelihood of outcomes within that range.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "2fLOVN-lZtzY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques4) What are probability distribution functions (PDF)?"
      ],
      "metadata": {
        "id": "LA7yi8XJbIJf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- A probability distribution function (PDF) is a mathematical function that describes the likelihood of a continuous random variable taking on a particular value within a given range. Unlike discrete variables, where probabilities are assigned to specific values, a PDF does not give the probability of the variable being exactly equal to a certain number—instead, it gives the relative likelihood of the variable falling within an interval. The probability of the variable being within a range is found by calculating the area under the curve of the PDF over that interval. A valid PDF must always be non-negative and the total area under the curve must equal one, ensuring that the sum of all probabilities over the possible values is 100%. PDFs are essential tools in continuous probability models and are widely used in areas such as physics, finance, and data science to represent distributions like the normal, exponential, or uniform distributions."
      ],
      "metadata": {
        "id": "WaJBG2x6bLaR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques5) How do cumulative distribution functions (CDF) differ from probability distribution functions (PDF)?"
      ],
      "metadata": {
        "id": "FY7X_UyTZ0Kb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Cumulative distribution functions (CDF) and probability distribution functions (PDF) serve different but related roles in describing the behavior of random variables. A probability distribution function—specifically, a probability density function (PDF) for continuous variables or a probability mass function (PMF) for discrete variables—shows the likelihood of the random variable taking on specific values (in the case of PMF) or the relative likelihood over intervals (in the case of PDF). In contrast, a cumulative distribution function (CDF) gives the probability that the random variable is less than or equal to a certain value. Essentially, the CDF is the cumulative sum or integral of the PDF or PMF up to a point, providing a full picture of the distribution up to that value. While the PDF/PMF focuses on individual outcomes or small ranges, the CDF shows accumulated probabilities, making it useful for assessing the likelihood of a variable falling within a certain range."
      ],
      "metadata": {
        "id": "iTDZEo2AZ4Xi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques6) What is a discrete uniform distribution?"
      ],
      "metadata": {
        "id": "Q7ICgKihZ_3x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- A discrete uniform distribution is a type of probability distribution in which all outcomes are equally likely within a finite set of distinct values. Each value has the same constant probability of occurring, making the distribution completely symmetrical. For example, when rolling a fair six-sided die, the probability of getting any one of the numbers from 1 to 6 is exactly 1/6. This equal likelihood of outcomes is the defining feature of a discrete uniform distribution. It is often used in situations where there is no inherent bias or preference for any particular outcome, and it provides a simple model for analyzing fairness and randomness in experiments."
      ],
      "metadata": {
        "id": "leGslNOTaDXp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques7) What are the key properties of a Bernoulli distribution?"
      ],
      "metadata": {
        "id": "e9AuQRVUaLaO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The Bernoulli distribution is a discrete probability distribution that models a random experiment with only two possible outcomes: success (usually coded as 1) and failure (usually coded as 0). One of its key properties is that it is defined by a single parameter,p,which represents the probability of success; the probability of failure is therefore (1-p) The mean (or expected value) of a Bernoulli distribution is p,and the variance is p(1-p). This distribution is often used to represent binary outcomes such as flipping a coin (heads or tails), answering a yes/no question, or detecting the presence or absence of a signal. Its simplicity makes it a building block for more complex distributions, such as the binomial distribution."
      ],
      "metadata": {
        "id": "yPD6QWFZaOpb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques8) What is the binomial distribution, and how is it used in probability?"
      ],
      "metadata": {
        "id": "u6Zrl8CialG7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The binomial distribution is a discrete probability distribution that describes the number of successes in a fixed number of independent trials, each with the same probability of success. It is defined by two parameters: n,the number of trials, and p,  the probability of success in a single trial. The distribution calculates the likelihood of obtaining exactly k successes in n trials  using the binomial formula. It is commonly used in situations where there are repeated, identical experiments with binary outcomes—such as flipping a coin multiple times, checking for defective items in a batch, or conducting surveys with yes/no responses. The binomial distribution helps model and analyze scenarios involving chance and proportions, making it widely applicable in fields like statistics, engineering, and medicine.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qez7bHFDaphC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques9) What is the Poisson distribution and where is it applied?"
      ],
      "metadata": {
        "id": "Z22xJtGWa8de"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The Poisson distribution is a discrete probability distribution that models the number of events occurring within a fixed interval of time or space, under the condition that these events happen independently and at a constant average rate. It is defined by a single parameter λ (lambda), which represents the average number of occurrences in the given interval. The Poisson distribution is particularly useful for modeling rare or infrequent events, such as the number of phone calls received by a call center in an hour, the number of typing errors on a page, or the arrival of customers at a service point. Its key characteristic is that it does not assume a fixed number of trials, unlike the binomial distribution, making it ideal for analyzing count data over time or space where the number of opportunities for events is not limited."
      ],
      "metadata": {
        "id": "0nqomDkMbekq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques10) What is a continuous uniform distribution?"
      ],
      "metadata": {
        "id": "0oF96B_lbpiZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- A continuous uniform distribution is a type of probability distribution in which all values within a certain interval are equally likely to occur. It is defined by two parameters, a and b which represent the minimum and maximum values of the interval, respectively. The probability density function (PDF) of a continuous uniform distribution is constant between a and b and zero outside this range. This means the distribution has a flat, rectangular shape when graphed. Since every outcome within the interval is equally probable, the probability of the random variable falling within a subinterval is proportional to the length of that subinterval. This distribution is commonly used in simulations, randomized sampling, and modeling scenarios where outcomes are uniformly distributed across a continuous range, such as the time it takes for a bus to arrive within a known time window.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "g16Ioey-bswE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques11) What are the characteristics of a normal distribution?"
      ],
      "metadata": {
        "id": "HclTikxIb6tm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The normal distribution is a continuous probability distribution that is symmetric and bell-shaped, often referred to as a Gaussian distribution. Its key characteristics include a mean (μ) that defines the center of the distribution, and a standard deviation (σ) that controls the spread of the distribution. The shape of the normal distribution curve is such that it is highest at the mean and tapers off symmetrically towards both extremes. About 68% of the data falls within one standard deviation of the mean, 95% within two standard deviations, and 99.7% within three standard deviations, which is often referred to as the \"68-95-99.7 rule.\" The normal distribution is important because it describes many natural phenomena, such as human heights, measurement errors, and IQ scores, and serves as the basis for many statistical methods, including hypothesis testing and confidence intervals. The central limit theorem also states that the distribution of the sum (or average) of a large number of independent, identically distributed random variables tends to be normal, regardless of the original distribution."
      ],
      "metadata": {
        "id": "rrCYkLxOb-Bs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques12)  What is the standard normal distribution, and why is it important?"
      ],
      "metadata": {
        "id": "omQidlr5cMfB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The standard normal distribution is a special case of the normal distribution where the mean is 0 and the standard deviation is 1. Its probability density function (PDF) follows the same bell-shaped curve as any normal distribution, but with these specific parameters. The standard normal distribution is important because it serves as a reference point for comparing other normal distributions. By converting data from any normal distribution into a standard normal distribution through a process called z-scoring, we can use a standardized scale to determine probabilities, make comparisons, and perform statistical analysis. The z-score represents how many standard deviations a data point is away from the mean. This transformation allows us to use standard tables (z-tables) or computational tools to quickly find probabilities associated with any normal distribution, making it fundamental in hypothesis testing, confidence intervals, and statistical inference."
      ],
      "metadata": {
        "id": "m6fh9FeecQM4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques13) What is the Central Limit Theorem (CLT), and why is it critical in statistics?"
      ],
      "metadata": {
        "id": "rpOXjm65cYT7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The Central Limit Theorem (CLT) is a fundamental concept in statistics that states that, regardless of the shape of the original population distribution, the distribution of the sample mean will approach a normal distribution as the sample size increases, provided the samples are independent and identically distributed. This holds true even if the population is not normally distributed. The CLT is critical because it allows statisticians to make inferences about population parameters using the normal distribution, even when the underlying population is not normal. As the sample size increases, the sample mean becomes more reliable as an estimate of the population mean, and the sampling distribution of the mean becomes increasingly normal. This makes the CLT a cornerstone of inferential statistics, enabling the use of confidence intervals, hypothesis tests, and other statistical methods that rely on the assumption of normality, even when the data itself might not be normally distributed."
      ],
      "metadata": {
        "id": "HrRrE0VPcbe8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques14) How does the Central Limit Theorem relate to the normal distribution?"
      ],
      "metadata": {
        "id": "SXVXmoKLcfxS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The Central Limit Theorem (CLT) is closely related to the normal distribution because it explains why the normal distribution is so prevalent in statistics, even when the data itself is not normally distributed. According to the CLT, when we take a large number of independent, random samples from any population with any distribution (not necessarily normal), the distribution of the sample means will approximate a normal distribution as the sample size increases, regardless of the original population’s distribution. This is significant because it means that even if the underlying data is skewed or irregular, the sampling distribution of the mean will follow a normal distribution as long as the sample size is sufficiently large (typically around 30 or more). Thus, the CLT provides the theoretical foundation for using the normal distribution in many statistical procedures, such as hypothesis testing and confidence intervals, even in situations where the original data is not normally distributed.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "dVbJ_cixci5b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques15) What is the application of Z statistics in hypothesis testing?"
      ],
      "metadata": {
        "id": "YkRkyvdacm75"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Z statistics are widely used in hypothesis testing to determine how far a sample mean is from the population mean, measured in terms of standard deviations. In hypothesis testing, a Z-test is typically applied when the population standard deviation is known, and the sample size is large (typically n>30). The Z statistic, or Z-score, is calculated by subtracting the population mean from the sample mean and then dividing by the standard error (which is the population standard deviation divided by the square root of the sample size). The resulting Z-score indicates how many standard deviations the sample mean is away from the population mean.\n",
        "\n",
        "  In hypothesis testing, the Z statistic is used to assess whether the observed data falls within a certain range of values under the null hypothesis. If the Z-score falls beyond a critical value (determined by the significance level,α),the null hypothesis is rejected, indicating that the sample provides enough evidence to support the alternative hypothesis. Z-tests are commonly applied in scenarios such as testing the effectiveness of a new treatment, comparing sample means, or determining if a sample proportion is significantly different from a known population proportion."
      ],
      "metadata": {
        "id": "Xa8XTaYAcqTZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques16) How do you calculate a Z-score, and what does it represent?-"
      ],
      "metadata": {
        "id": "HUrwO83oc4_i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- A Z-score is a statistical measurement that describes how many standard deviations a data point is away from the mean of the data set. It is calculated by subtracting the population mean (𝜇) from the individual data point (X) and then dividing the result by the population standard deviation (σ). The formula for calculating a Z-score is:\n",
        "\n",
        "  z = (X -μ)/σ\n",
        "  \n",
        "  Where:\n",
        "  X is the individual data point,\n",
        "  μ is the population mean, and\n",
        "  σ is the population standard deviation.\n",
        "\n",
        "  The Z-score tells you whether the data point is above or below the mean and how far away it is in terms of standard deviations. A positive Z-score indicates the value is above the mean, while a negative Z-score indicates it is below the mean. A Z-score of 0 means the data point is exactly equal to the mean. Z-scores are commonly used to standardize data, compare values from different distributions, and conduct hypothesis tests, as they allow for a direct comparison between different datasets, regardless of their units or scales."
      ],
      "metadata": {
        "id": "ptTr5z-4c_-M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques17) What are point estimates and interval estimates in statistics?"
      ],
      "metadata": {
        "id": "KvAb2bNGdf7f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- In statistics, point estimates and interval estimates are two types of estimates used to infer population parameters based on sample data. A point estimate provides a single value as an estimate of an unknown population parameter, such as the sample mean or sample proportion. For example, if we want to estimate the average height of a population, the sample mean would serve as a point estimate of the population mean. While point estimates are simple and straightforward, they do not provide any information about the uncertainty or variability of the estimate.\n",
        "\n",
        "  On the other hand, interval estimates offer a range of values within which the population parameter is likely to fall, providing a measure of the uncertainty around the point estimate. The most common form of interval estimate is the confidence interval, which specifies a range along with a level of confidence (e.g., 95% confidence interval) that the true parameter lies within that range. Interval estimates are generally preferred in statistical analysis because they account for sample variability and provide more context about the precision of the estimate, making them more informative than point estimates alone."
      ],
      "metadata": {
        "id": "4XJGhzeNdjlN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques18) What is the significance of confidence intervals in statistical analysis?"
      ],
      "metadata": {
        "id": "1mO6bISjdqYJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Confidence intervals (CIs) play a crucial role in statistical analysis by providing a range of plausible values for a population parameter, along with an associated level of confidence that the true parameter lies within this range. A confidence interval is typically expressed as a range (e.g., 95% CI: 4.5 to 6.5), where the interval is constructed from sample data. The level of confidence (such as 95% or 99%) indicates the likelihood that the interval contains the true population parameter if the sampling process were repeated multiple times.\n",
        "\n",
        "  The significance of confidence intervals lies in their ability to quantify uncertainty and variability in estimates. Unlike point estimates, which offer a single value without accounting for error or uncertainty, confidence intervals give a broader context by acknowledging that sample data is just an estimate of the true population parameter. By providing a range, CIs allow researchers and decision-makers to assess the precision of the estimate and make more informed conclusions. For example, in hypothesis testing or estimating population means, confidence intervals can help determine whether an observed effect is statistically significant and whether it’s likely to be meaningful in the population."
      ],
      "metadata": {
        "id": "1PtkOVjzdtJ6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques19) What is the relationship between a Z-score and a confidence interval?"
      ],
      "metadata": {
        "id": "pIAY0hfUdx8d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The relationship between a Z-score and a confidence interval lies in the way both are used to estimate population parameters and assess the uncertainty of those estimates. A Z-score represents the number of standard deviations a sample statistic (such as a sample mean) is away from the population mean. In the context of a confidence interval, the Z-score is used to determine the critical value that defines the bounds of the interval.\n",
        "\n",
        "  When constructing a confidence interval, particularly for large sample sizes, the Z-score corresponds to the desired confidence level. For example, for a 95% confidence level, the Z-score is typically 1.96, meaning that 95% of the data falls within 1.96 standard deviations of the population mean. The confidence interval is calculated by taking the sample mean and adding and subtracting the product of the Z-score and the standard error of the mean. This interval gives a range within which the true population parameter is likely to fall, with the Z-score helping to set the width of this range based on the chosen confidence level. Therefore, the Z-score plays a critical role in determining the precision and reliability of the confidence interval in inferential statistics.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "XCs6Os9Md1sw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques20)  How are Z-scores used to compare different distributions?"
      ],
      "metadata": {
        "id": "s2lI10WTd7zq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Z-scores are used to compare different distributions by standardizing data from those distributions, allowing for a common scale despite differences in their means and standard deviations. When data from different distributions are transformed into Z-scores, they are expressed in terms of how many standard deviations they are away from their respective means. This standardization allows for direct comparison of data points or sample statistics from distributions that may have different units, scales, or underlying distributions.\n",
        "\n",
        "  For example, if one distribution has a mean of 50 and a standard deviation of 10, and another has a mean of 100 and a standard deviation of 20, comparing raw data points between these two distributions would be difficult. However, by converting the data points to Z-scores, both distributions are scaled to a standard normal distribution with a mean of 0 and a standard deviation of 1. This allows for meaningful comparisons between data points, regardless of the original scales or units. Z-scores also help in comparing individual data points from different distributions to determine which one is more extreme or how they relate to their respective populations.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ko9xCdKWd_3e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques21) What are the assumptions for applying the Central Limit Theorem?"
      ],
      "metadata": {
        "id": "w4C9INQUeFmN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The Central Limit Theorem (CLT) is a powerful statistical principle, but its application relies on a few key assumptions. First, the data must consist of independent observations, meaning the outcome of one observation does not influence another. Second, the sample size should be sufficiently large, typically n>= 30 especially when the population distribution is not normal, as this allows the sampling distribution of the sample mean to approximate a normal distribution. If the population is already normal, smaller sample sizes can often be used. Third, the samples should be identically distributed, meaning each observation is drawn from the same population and follows the same probability distribution. Finally, the population must have a finite variance—meaning the data should not have infinite variance or extreme outliers that could distort the results. When these assumptions are met, the CLT ensures that the sampling distribution of the sample mean approaches a normal distribution, regardless of the original population's distribution.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "LUCG1GxIeIw_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques22) What is the concept of expected value in a probability distribution?"
      ],
      "metadata": {
        "id": "jK2ARJjqeRJ5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The expected value in a probability distribution is a measure of the central tendency, representing the long-term average or mean value of a random variable if an experiment were repeated infinitely under identical conditions. It is often referred to as the \"mean\" or \"average\" of the distribution, though it doesn't necessarily correspond to an actual outcome but rather to a weighted average of all possible values, weighted by their probabilities. For a discrete random variable, the expected value is calculated by summing the products of each possible value of the variable and its corresponding probability. For a continuous random variable, the expected value is the integral of the variable’s value weighted by its probability density function (PDF). The expected value helps in making predictions or decisions based on probabilistic outcomes, such as predicting the average return on an investment or the expected score in a game, providing a foundational concept in statistics, economics, and decision theory."
      ],
      "metadata": {
        "id": "WeHDQGLIeT79"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques23)  How does a probability distribution relate to the expected outcome of a random variable?"
      ],
      "metadata": {
        "id": "0U2Jq-mQeYhD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- A probability distribution describes the likelihood of all possible outcomes of a random variable, and it directly influences the expected outcome (or expected value) of that variable. The expected value is essentially a weighted average of all possible values of the random variable, where each value is weighted by its probability as described by the probability distribution. In other words, the expected value gives a single value that represents the \"center\" or average of the distribution, reflecting what one would anticipate as the long-term outcome of the random variable if the process were repeated many times. For a discrete random variable, this involves multiplying each outcome by its probability and summing the results. For a continuous random variable, it involves integrating the product of the variable’s value and its probability density function (PDF). Therefore, the probability distribution is crucial because it defines how outcomes are spread and the likelihood of each, which directly determines the expected value."
      ],
      "metadata": {
        "id": "-GDtPLozecDe"
      }
    }
  ]
}